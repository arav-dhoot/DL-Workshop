{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arav-dhoot/DL-Workshop/blob/main/Celebrities_Classification_Algorithm_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b912bed5"
      },
      "source": [
        "#**Using Machine Learning to Detect Celebrities**\n",
        "\n",
        "For an average film-goer, differentiating between two celebrities is child's play. However, most computers need help understanding the difference. By building basic machine learning models, known as Convolutional Neural Networks, using basic Python libraries such as numpy, pandas, tensorflow, keras, and scikit-learn, we will train a computer to learn the difference between different celebrities."
      ],
      "id": "b912bed5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SyB9XLbBYo5"
      },
      "source": [
        "This notebook is common for all students. So, before you start, please ensure that you make a copy of this notebook on your own Google Drive. To make a copy of this notebook use the following tools on the top left-hand corner: **File > Save a copy in Drive**"
      ],
      "id": "-SyB9XLbBYo5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-10TPKmo4KE"
      },
      "outputs": [],
      "source": [
        "#These packages help with file management\n",
        "import os\n",
        "import glob as gb\n",
        "from google.colab import files"
      ],
      "id": "p-10TPKmo4KE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2tzH_gLChYv"
      },
      "source": [
        "##Numpy\n",
        "* NumPy is a popular Python library for numerical computing and array manipulation.\n",
        "* It provides powerful tools for efficient numerical operations on multi-dimensional arrays and matrices.\n",
        "* NumPy's core feature is its ndarray (N-dimensional array) object, which allows for fast and memory-efficient array operations.\n",
        "* It offers a wide range of mathematical functions for array manipulation, such as linear algebra, Fourier transforms, and random number generation.\n",
        "* NumPy is widely used in scientific computing, data analysis, machine learning, and other domains that require high-performance numerical operations."
      ],
      "id": "T2tzH_gLChYv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k_LPBHVCqly"
      },
      "source": [
        "##Pandas\n",
        "* Pandas is a powerful Python library for data manipulation, analysis, and exploration.\n",
        "* It provides easy-to-use data structures, such as DataFrame and Series, for efficient handling of structured data.\n",
        "* Pandas enables loading data from various sources like CSV, Excel, SQL databases, and more, allowing for seamless data ingestion and integration.\n",
        "* It offers extensive functionality for data cleaning, transformation, and preprocessing, including missing data handling, data alignment, and data type conversion.\n",
        "* Pandas supports flexible data indexing, slicing, and filtering operations, making it convenient for extracting and manipulating subsets of data."
      ],
      "id": "4k_LPBHVCqly"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxA_aRV5GLZb"
      },
      "source": [
        "##Matplotlib\n",
        "* Matplotlib is a popular data visualization library in Python.\n",
        "* It provides a comprehensive set of functions for creating various types of plots, charts, and graphs.\n",
        "* Matplotlib allows for the creation of static, animated, and interactive visualizations with high customization options.\n",
        "* It supports a wide range of plot types, including line plots, scatter plots, bar plots, histograms, pie charts, and more.\n",
        "* Matplotlib provides fine-grained control over plot elements such as axes, labels, titles, colors, and styles."
      ],
      "id": "pxA_aRV5GLZb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbU0Z9wlGb4P"
      },
      "source": [
        "##Seaborn\n",
        "* Seaborn is a powerful data visualization library built on top of Matplotlib in Python.\n",
        "* It provides a higher-level interface and simplifies the process of creating aesthetically pleasing statistical graphics.\n",
        "* Seaborn offers a wide range of statistical visualization functions, including scatter plots, line plots, bar plots, histograms, box plots, heatmaps, and more.\n",
        "* It specializes in creating informative and visually appealing visualizations with minimal code."
      ],
      "id": "lbU0Z9wlGb4P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np_Br4NQpBXv"
      },
      "outputs": [],
      "source": [
        "#These packages help with data visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "id": "np_Br4NQpBXv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTOrFUuAInrh"
      },
      "source": [
        "## Pillow\n",
        "* Pillow is a popular Python library for image processing and manipulation.\n",
        "* It provides a wide range of functions and methods for opening, editing, and saving various image file formats.\n",
        "* Pillow allows for basic image operations such as resizing, cropping, rotating, and flipping images.\n",
        "* It supports advanced image processing tasks, including filtering, blending, enhancing, and applying effects to images."
      ],
      "id": "rTOrFUuAInrh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4nAwSYpJA4x"
      },
      "source": [
        "## OpenCV\n",
        "* OpenCV (Open Source Computer Vision Library) is a popular open-source computer vision and image processing library.\n",
        "* It provides a wide range of functions and algorithms for tasks related to image and video analysis, object detection, and machine learning.\n",
        "* OpenCV supports real-time computer vision applications and can process images and videos in real-time from various sources.\n",
        "* It offers functionalities such as image and video capture, image filtering, feature detection, object recognition, and camera calibration.\n",
        "* OpenCV provides efficient implementations of common computer vision algorithms, including edge detection, image segmentation, and optical flow."
      ],
      "id": "r4nAwSYpJA4x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B425MZWEpGg-"
      },
      "outputs": [],
      "source": [
        "#These packages help with managing and viewing images\n",
        "from PIL import Image\n",
        "import cv2"
      ],
      "id": "B425MZWEpGg-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9zE2nSNJNIL"
      },
      "source": [
        "## Tensorflow\n",
        "* TensorFlow is a popular open-source machine learning framework developed by Google.\n",
        "* It provides a flexible and scalable ecosystem for building and deploying machine learning models.\n",
        "* TensorFlow offers a computational graph abstraction, where models are represented as a series of connected nodes that perform mathematical operations.\n",
        "* It supports both deep learning and traditional machine learning algorithms, allowing for a wide range of applications.\n",
        "* TensorFlow provides extensive tools and libraries for tasks such as neural networks, natural language processing, computer vision, and reinforcement learning."
      ],
      "id": "l9zE2nSNJNIL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe1lfGzyKcSb"
      },
      "source": [
        "## Scikit-Learn\n",
        "* Scikit-learn is a popular open-source machine learning library for Python.\n",
        "* It provides a comprehensive set of tools and algorithms for machine learning tasks, including classification, regression, clustering, dimensionality reduction, and model evaluation.\n",
        "* Scikit-learn offers a consistent and user-friendly API, making it easy to develop and experiment with machine learning models.\n",
        "* It supports various supervised and unsupervised learning techniques, such as decision trees, support vector machines, random forests, k-means clustering, and principal component analysis."
      ],
      "id": "Fe1lfGzyKcSb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw2eXzQypUnj"
      },
      "outputs": [],
      "source": [
        "#These packages help with building, training, and analyzing the CNN\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, GlobalAveragePooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "tw2eXzQypUnj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym25m1rLU2zf"
      },
      "source": [
        "When you run the next code block, you will be prompted to upload files. Please upload the ```M17.h5``` file to the notebook. That file is essential in running further code blocks."
      ],
      "id": "ym25m1rLU2zf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C0iAkakgTIrP"
      },
      "outputs": [],
      "source": [
        "files.upload()"
      ],
      "id": "C0iAkakgTIrP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSFbQrPoVJCU"
      },
      "source": [
        "In the next code block, you will be prompted to give Google Colab access to your Google Drive. Please ensure that you give Colab access to the Google account that you have stored the ```Celebrity_Faces_Dataset``` in."
      ],
      "id": "ZSFbQrPoVJCU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27CpDWCGNXfg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "27CpDWCGNXfg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCXnfXShY_LE"
      },
      "source": [
        "The network we train can only accept an image of the same size. All images need to be resized so that they are the same size. Having a smaller image results in a CNN that uses fewer computational resources during training. With a larger image, on the other hand, you retain a greater amount of information about the image. In the next block, you can enter a numeric value that you want the images to be resized to."
      ],
      "id": "GCXnfXShY_LE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G1ClMugZW4y"
      },
      "outputs": [],
      "source": [
        "ImageSize = 150 #@param {type:\"slider\", min:100, max:224, step:2}"
      ],
      "id": "8G1ClMugZW4y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6b284fe3",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#Declaring constants.\n",
        "PATH='/content/drive/MyDrive/Celebrity_Faces_Dataset' #Change the path so that it points to the celebrity dataset on your Drive."
      ],
      "id": "6b284fe3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "extra-height",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#Preliminary data visualization. It gives us a general idea of the distribution of the files between the different classes.\n",
        "i = 0\n",
        "for folder in os.listdir(PATH):\n",
        "    files = gb.glob(pathname= str(f'{PATH}/{folder}/*.jpg'))\n",
        "    print(f'There are {len(files)} images in {folder} folder')\n",
        "    i+=1\n",
        "print(f'There are a total of {i} folders!')"
      ],
      "id": "extra-height"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sensitive-dairy"
      },
      "outputs": [],
      "source": [
        "# Define a data dictionnary mapping each celebrity name\n",
        "# to \"its class\", a numerical value representing the celebrity\n",
        "# The dictionnnary allows to easily go from the numeric value to the corresponding name\n",
        "# and vice-verca\n",
        "# Numeric values are easier to handle by the DL algorithms we shall build\n",
        "# So we have 17 celebrities and thus 17 classes: from 0 to 16\n",
        "CelebCodes = {'Sandra_Bullock':0,\n",
        "        'Angelina_Jolie':1,\n",
        "        'Natalie_Portman':2,\n",
        "        'Megan_Fox':3,\n",
        "        'Tom_Cruise':4,\n",
        "        'Kate_Winslet':5,\n",
        "        'Leonardo_DiCaprio':6,\n",
        "        'Jennifer_Lawrence':7,\n",
        "        'Brad_Pitt':8,\n",
        "        'Hugh_Jackman':9,\n",
        "        'Will_Smith':10,\n",
        "        'Nicole_Kidman':11,\n",
        "        'Johnny_Depp':12,\n",
        "        'Robert_Downey_Jr':13,\n",
        "        'Tom_Hanks':14,\n",
        "        'Scarlett_Johansson':15,\n",
        "        'Denzel_Washington':16}\n",
        "CelebKeys = list(CelebCodes.keys())"
      ],
      "id": "sensitive-dairy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chemical-arctic"
      },
      "outputs": [],
      "source": [
        "#The X is a list of pictures in the form numpy arrays (celebrity images).\n",
        "X = []\n",
        "#The y is a list of celebrity classes of the corresponding images.\n",
        "y = []\n",
        "for folder in os.listdir(PATH):\n",
        "    files = gb.glob(pathname= str(f'{PATH}/{folder}/*.jpg'))\n",
        "    for file in files:\n",
        "        image = cv2.imread(file)\n",
        "        image_array = cv2.resize(image, (ImageSize,ImageSize)) #Each of the images is resized to a uniform 224 x 224 size.\n",
        "        X.append(list(image_array))\n",
        "        y.append(CelebCodes[folder])"
      ],
      "id": "chemical-arctic"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwASwlATjWj5"
      },
      "source": [
        "## Data Visualization\n",
        "\n",
        "Now let's view a few images to see what our data looks like. In the next code block, you can input any number between 0 and 1699. The number you choose is an index and corresponds to a specific celebrity. Enter a few number and run the cell to view different celebrites. What do you notice when you enter consecutive numbers? What do you notice when you enter numbers with a difference of 100 or more?"
      ],
      "id": "IwASwlATjWj5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FIfYyinyt5Q3"
      },
      "outputs": [],
      "source": [
        "index = 1601 #@param {type:\"integer\"}\n",
        "if index < len(X):\n",
        "  plt.imshow(X[index])\n",
        "  title = str(f'Class: {y[index]:02} -- Celebrity: {CelebKeys[y[index]]}')\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "else:\n",
        "  print('Index out of range!')"
      ],
      "id": "FIfYyinyt5Q3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djtwQOmBWe2l"
      },
      "source": [
        "## One-Hot Encoding\n",
        "One-hot encoding is a way to represent categorical data in a format that a computer can understand. Imagine you have a list of categories, like colors: red, blue, and green. Instead of using words to represent these categories, we convert them into numbers.\n",
        "\n",
        "In one-hot encoding, we create a new column for each category. If an item belongs to a specific category, we put a \"1\" in that column; otherwise, we put a \"0\". So, for our example of colors, the column for red would have a \"1\" if an item is red and \"0\" otherwise. The column for blue would have a \"1\" if an item is blue and \"0\" otherwise, and so on."
      ],
      "id": "djtwQOmBWe2l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "disturbed-butterfly"
      },
      "outputs": [],
      "source": [
        "#Each value in the y array is converted to a one-hot encoded array.\n",
        "encoder = OneHotEncoder()\n",
        "Y = encoder.fit_transform(np.array(y).reshape(-1,1)).toarray()"
      ],
      "id": "disturbed-butterfly"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "controlling-smile"
      },
      "outputs": [],
      "source": [
        "#The X and Y lists are converted to numpy arrays.\n",
        "X = np.asarray(X)\n",
        "Y = np.asarray(Y)"
      ],
      "id": "controlling-smile"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "indian-energy"
      },
      "outputs": [],
      "source": [
        "#The X and y arrays are split into train and test sub-datasets. Their ratio is 7:3 respectively.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1 , shuffle=True)\n",
        "\n",
        "#Visualizing the train and test datasets.\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "print(y_train.shape)"
      ],
      "id": "indian-energy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD97RrjjZ4jT"
      },
      "source": [
        "###Batch Size\n",
        "In machine learning, batch size refers to the number of data samples that are processed together in one iteration during the training of a model. It affects how the model learns from the data and how computational resources are utilized.\n",
        "\n",
        "Imagine you have a large set of math problems to solve. Instead of solving them one by one, you decide to solve a group of problems together. The batch size is like the number of problems you solve in each round.\n",
        "\n",
        "Once you have the trained the model, you can come back and play with the value of the batch size and change it until you get a better test accuracy score."
      ],
      "id": "DD97RrjjZ4jT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8bQvF6ZubPGR"
      },
      "outputs": [],
      "source": [
        "BatchSize = 20 #@param {type:\"slider\", min:10, max:30, step:1}"
      ],
      "id": "8bQvF6ZubPGR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEhV9sxeWrW0"
      },
      "source": [
        "## Data Augmentation\n",
        "Data augmentation is a technique used to increase the size and diversity of a dataset by applying various transformations to existing data samples. It helps in training machine learning models to perform better and generalize well on unseen data.\n",
        "\n",
        "Imagine you have a small set of images to train a model, but you want to make it more robust and capable of recognizing different variations of those images. Instead of collecting more images, you can use data augmentation.\n",
        "\n",
        "Data augmentation involves applying operations like flipping, rotating, scaling, cropping, or changing the colors of the existing images. By doing this, you create new versions of the same image with slight modifications."
      ],
      "id": "yEhV9sxeWrW0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Vh10BR24fB2"
      },
      "outputs": [],
      "source": [
        "#Creating a list of transformations that we will apply to each image. This process is known as image augmentation. It helps to make deep learning algorithms robust.\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255, #Converts each pixel value in the image array to a decimal value.\n",
        "                                   shear_range = 20.0, #Slants the image by the specified value.\n",
        "                                   rotation_range = 20, #Rotates the image by the specied value.\n",
        "                                  )\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255) #Converts each pixel value in the image array to a decimal value.\n",
        "\n",
        "#Applying the transformations to all the images.\n",
        "train_iterator = train_datagen.flow(X_train, y_train, BatchSize)\n",
        "test_iterator = test_datagen.flow(X_test, y_test, BatchSize)\n",
        "print('Batches train=%d, test=%d, batch-size=%d' % (len(train_iterator), len(test_iterator), BatchSize)) #The ratio of the batch size of the train dataset to the test dataset is about 7:3.\n",
        "batchX, batchy = train_iterator.next()\n"
      ],
      "id": "4Vh10BR24fB2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnT8te_E6qxP"
      },
      "source": [
        "##**Building the Model**##"
      ],
      "id": "XnT8te_E6qxP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgKdWsWobrc9"
      },
      "source": [
        "###Question Time!!"
      ],
      "id": "rgKdWsWobrc9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "squA5oXTcHcI"
      },
      "source": [
        "1. What should the input shape of the first convolution layer be? *Hint: the input shape is the same as the size of the image.*"
      ],
      "id": "squA5oXTcHcI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TuiCFQ0LcbZS"
      },
      "outputs": [],
      "source": [
        "x_axis_size = 224 #@param {type:\"integer\"}\n",
        "y_axis_size = 224 #@param {type:\"integer\"}"
      ],
      "id": "TuiCFQ0LcbZS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpDQK3o1OUmy"
      },
      "source": [
        " Run this code block to find out if your answer correct"
      ],
      "id": "fpDQK3o1OUmy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8pYramk_NuGQ"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "if x_axis_size != ImageSize or y_axis_size != ImageSize:\n",
        "  print(f'Oh no! The input size needs to be the same as the image size: {ImageSize}')\n",
        "else:\n",
        "  print('Yay! You are correct!')"
      ],
      "id": "8pYramk_NuGQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8SaaAyRcsz7"
      },
      "source": [
        "2. How many neurons (outputs) should the Dense, or final, layer of the model have? *Hint: the model should have as many output neuros as the number of classes of the data*"
      ],
      "id": "e8SaaAyRcsz7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Nl89D9P2dIHd"
      },
      "outputs": [],
      "source": [
        " output_neuron = 17 #@param {type:\"integer\"}"
      ],
      "id": "Nl89D9P2dIHd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k12WCXiLOY5Z"
      },
      "source": [
        "Run this code block to find out if your answer is correct"
      ],
      "id": "k12WCXiLOY5Z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Mr-0x7GXOc3b"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "if output_neuron != len(CelebKeys):\n",
        "  print('Oh no! The number of output neurons need to be the same as the number of classes we are detecting: 17')\n",
        "else:\n",
        "  print('Yay! Your answer is correct!')"
      ],
      "id": "Mr-0x7GXOc3b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sexual-theology"
      },
      "outputs": [],
      "source": [
        "#Creating our CNN model!!!\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(ImageSize,ImageSize,3)))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(Conv2D(32,kernel_size=(4,4),activation='relu'))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(Dropout(rate=0.5))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(len(CelebKeys),activation='softmax'))"
      ],
      "id": "sexual-theology"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw_tcpyWdRN8"
      },
      "source": [
        "### Learning Rate\n",
        "The learning rate is a parameter used in training machine learning models, specifically in optimization algorithms like gradient descent. It determines the step size taken in adjusting the model's parameters during the learning process.\n",
        "\n",
        "Imagine you are climbing a mountain, and you want to reach the top by taking small steps. The learning rate is like the size of the steps you take. If the learning rate is too large, you might overshoot the optimal point and miss the peak. On the other hand, if the learning rate is too small, you might take forever to reach the top or get stuck in a suboptimal position.\n",
        "\n",
        "In the next block, you can choose the learning rate. Don't forget to come back and play with the learning rate."
      ],
      "id": "Gw_tcpyWdRN8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GURTGNHld_89"
      },
      "outputs": [],
      "source": [
        "LearningRate = 0.0717236 #@param {type:\"slider\", min:0.0000001, max:0.1, step:1e-8}"
      ],
      "id": "GURTGNHld_89"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "widespread-enclosure"
      },
      "outputs": [],
      "source": [
        "#Creating a Adam optimizer, with a learning rate you have just chosen. The learning rate is a hyperparameter that can be adjusted to improve the performance of the model.\n",
        "opt = tf.keras.optimizers.Adam(LearningRate)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "id": "widespread-enclosure"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "following-madagascar"
      },
      "outputs": [],
      "source": [
        "#Visualizing the model.\n",
        "print('Model Details are : ')\n",
        "print(model.summary())"
      ],
      "id": "following-madagascar"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgT6V_4ZX_ls"
      },
      "source": [
        "## Epochs\n",
        "In machine learning, an epoch refers to a complete pass through the entire training dataset during the model training process. It helps in determining how many times the model will iteratively learn from the training data.\n",
        "\n",
        "Imagine you have a book that you want to read and understand thoroughly. Instead of reading it all at once, you decide to read it chapter by chapter. Each time you read through all the chapters of the book, you complete one epoch.\n",
        "\n",
        "In the next block, you can choose the number of epochs you want to train your model for. Don't forget to come back and play with the epoch."
      ],
      "id": "lgT6V_4ZX_ls"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "a30ji-YWYJZ-"
      },
      "outputs": [],
      "source": [
        "NumEpochs = 10 #@param {type:\"slider\", min:2, max:20, step:1}"
      ],
      "id": "a30ji-YWYJZ-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "common-egypt",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#Train the model for 80 epochs. The number of epochs can be increased, which usually improves the performance of the model.\n",
        "History = model.fit(X_train, y_train,  steps_per_epoch=len(train_iterator), epochs=NumEpochs)"
      ],
      "id": "common-egypt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtCFhYYAEsLC"
      },
      "outputs": [],
      "source": [
        "#Testing the model on the testing dataset.\n",
        "test_eval = model.evaluate(X_test, y_test, batch_size=BatchSize)\n",
        "print('Test loss:', test_eval[0])\n",
        "print(f'Test accuracy: {test_eval[1] * 100}%')"
      ],
      "id": "dtCFhYYAEsLC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaSTJ6H6y-88"
      },
      "source": [
        "Our contructed model does not receive a very high accuracy score because it has only been trained for a few epochs, and because it has a small brain. To improve the accuracy of a model, you may choose to increase it size (number of layers and structure), and change its hyperparameters to improve performance. For example, you may choose to modify the learning rate, or increase the number of epochs that you train the CNN for.\n",
        "\n",
        "In the next section, you will go against a bigger model that has been trained for 500 epochs and achieves an accuracy score that is very close to 100%.\n"
      ],
      "id": "JaSTJ6H6y-88"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBJNmQVNyU-a"
      },
      "outputs": [],
      "source": [
        "#Let us load a pretrained model that has good accuracy\n",
        "#Load the file where the model is stored\n",
        "PATHModel= '/content/M17.h5'\n",
        "#\n",
        "#load the model so we can work with it\n",
        "Model500 = keras.models.load_model(PATHModel)"
      ],
      "id": "qBJNmQVNyU-a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cc7At5OyvAY"
      },
      "outputs": [],
      "source": [
        "#Testing the loaded model on the testing dataset.\n",
        "test_eval = Model500.evaluate(X_test, y_test, batch_size=BatchSize)\n",
        "#Predicting values on the test dataset.\n",
        "pred_test = Model500.predict(X_test)\n",
        "y_pred = encoder.inverse_transform(pred_test)\n",
        "y_t = encoder.inverse_transform(y_test)\n",
        "print('Test loss:', test_eval[0])\n",
        "print(f'Test accuracy: {test_eval[1] * 100}%')"
      ],
      "id": "7cc7At5OyvAY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyOIcZ1oXRv9"
      },
      "source": [
        "Your model does not receive a very high accuracy score because it has only been trained for a few epochs. To improve the accuracy of a model, you may choose to change its hyperparameters to improve performance. For example, you may choose to modify the learning rate, or increase the number of epochs that you train the CNN for.\n",
        "\n",
        "In the next section, you will go against a model that has been trained for 500 epochs and achieves an accuracy score that is very close to 100%."
      ],
      "id": "FyOIcZ1oXRv9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNZArzruCn1l"
      },
      "source": [
        "**Now let's test you against a computer. You will be shown an image, choose the name of the celebrity, and then let's see what the model we just trained thinks. Run the next code block as many times as you wish.**"
      ],
      "id": "tNZArzruCn1l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7UI_BKRLDwA9"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import random\n",
        "random_integer=random.randint(0, len(os.listdir(PATH)))\n",
        "random_celebrity=f'{PATH}/{os.listdir(PATH)[random_integer]}'\n",
        "file = f'{random_celebrity}/{os.listdir(random_celebrity)[random.randint(0, len(os.listdir(random_celebrity)))]}'\n",
        "img = Image.open(file)\n",
        "img"
      ],
      "id": "7UI_BKRLDwA9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xWlO8bgSHgTu"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "YourChoice = 'Scarlett_Johansson' #@param ['Will_Smith','Scarlett_Johansson','Nicole_Kidman','Denzel_Washington','Johnny_Depp','Robert_Downey_Jr','Hugh_Jackman','Jennifer_Lawrence','Brad_Pitt','Leonardo_DiCaprio','Megan_Fox','Kate_Winslet','Angelina_Jolie','Tom_Cruise','Tom_Hanks','Sandra_Bullock','Natalie_Portman', 'Scarlett Johanson']"
      ],
      "id": "xWlO8bgSHgTu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vrFdUL-WJth8"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def get_celebrity(n):\n",
        "  for k, v in CelebCodes.items():\n",
        "    if v == n: return k\n",
        "\n",
        "def predict(img, model):\n",
        "  img = img.resize((ImageSize, ImageSize), Image.LANCZOS)\n",
        "  np_img = np.asarray(img)\n",
        "  return get_celebrity(np.argmax(model.predict(np.expand_dims(np_img, axis=0))))"
      ],
      "id": "vrFdUL-WJth8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhYIp_DoXKdT"
      },
      "outputs": [],
      "source": [
        "#predict using both model\n",
        "GoodModelValue = predict(img, Model500)\n",
        "BadModelValue = predict(img, model)"
      ],
      "id": "zhYIp_DoXKdT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FLXiKHVsYfyD"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "random_celebrity = random_celebrity.split('/')[-1]\n",
        "print(f'The good pre-trained model predicted celebrity: {GoodModelValue}')\n",
        "if GoodModelValue == random_celebrity:\n",
        "    if YourChoice != random_celebrity:\n",
        "      print('Oops! The model predicted correctly, but unfortunately, you did not')\n",
        "    elif YourChoice== random_celebrity:\n",
        "      print('Yay! Both you and the model predicted the celebrity correctly')\n",
        "else:\n",
        "    if YourChoice != random_celebrity:\n",
        "      print('Oops! Both you and the model predicted incorrectly')\n",
        "    elif YourChoice == random_celebrity:\n",
        "      print('Yay! You predicted correctly, but unfortunately, the model did not')\n",
        "\n",
        "print(f'The model that we just trained predicted celebrity is: {BadModelValue}')"
      ],
      "id": "FLXiKHVsYfyD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "convinced-differential"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#Placing all the values from the model prediction onto a dataframe.\n",
        "df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
        "df['Predicted Labels'] = y_pred.flatten()\n",
        "df['Actual Labels'] = y_t.flatten()"
      ],
      "id": "convinced-differential"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "registered-cannon"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "#Creating a confusion matrix. This helps visualize what the model predicted versus what the actual value of the images were.\n",
        "cm  = confusion_matrix(y_t, y_pred ,normalize='true')\n",
        "plt.figure(figsize = (12, 10))\n",
        "\n",
        "CNN=[cm[0,0],cm[1,1],cm[2,2],cm[3,3],cm[4,4]]\n",
        "CNN= pd.DataFrame(CNN)\n",
        "\n",
        "x_label=np.array(CelebKeys)\n",
        "y_label=np.array(CelebKeys)\n",
        "\n",
        "cmYP = pd.DataFrame(cm, index = x_label, columns = y_label,)\n",
        "\n",
        "sns.heatmap(cmYP, linecolor='white', cmap='Blues', linewidth=2, annot=True, )\n",
        "plt.title('Confusion Matrix', size=20)\n",
        "plt.xlabel('Predicted Labels', size=14)\n",
        "plt.ylabel('Actual Labels', size=14)\n",
        "plt.show()"
      ],
      "id": "registered-cannon"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}